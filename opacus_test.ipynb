{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Saad\\anaconda3\\envs\\tenv\\Lib\\site-packages\\datasets\\load.py:1429: FutureWarning: The repository for dair-ai/emotion contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/dair-ai/emotion\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\Saad\\anaconda3\\envs\\tenv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "c:\\Users\\Saad\\anaconda3\\envs\\tenv\\Lib\\site-packages\\opacus\\privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Saad\\anaconda3\\envs\\tenv\\Lib\\site-packages\\opacus\\accountants\\analysis\\rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Saad\\anaconda3\\envs\\tenv\\Lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "c:\\Users\\Saad\\anaconda3\\envs\\tenv\\Lib\\site-packages\\torch\\_functorch\\deprecated.py:100: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.make_functional is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.func.functional_call instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
      "  warn_deprecated('make_functional', 'torch.func.functional_call')\n",
      "c:\\Users\\Saad\\anaconda3\\envs\\tenv\\Lib\\site-packages\\torch\\_functorch\\deprecated.py:65: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.grad is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.func.grad instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
      "  warn_deprecated('grad')\n",
      "c:\\Users\\Saad\\anaconda3\\envs\\tenv\\Lib\\site-packages\\torch\\_functorch\\deprecated.py:61: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
      "  warn_deprecated('vmap', 'torch.vmap')\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from opacus import PrivacyEngine\n",
    "import torch\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from opacus.utils.uniform_sampler import UniformWithReplacementSampler\n",
    "# Load GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({'sep_token': '[SEP]'})\n",
    "tokenizer.add_special_tokens({'bos_token': '[BOS]'})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "LABELS = [\n",
    "    \"sadness\",\n",
    "    \"joy\",\n",
    "    \"love\",\n",
    "    \"anger\",\n",
    "    \"fear\",\n",
    "    \"suprise\"\n",
    "]\n",
    "dataset = load_dataset(\"dair-ai/emotion\", split=\"train\")\n",
    "test_dataset = load_dataset(\"dair-ai/emotion\", split=\"test\")\n",
    "MAX_LEN = 128\n",
    "input_ids = [\n",
    "    tokenizer.encode(\n",
    "        t, add_special_tokens=True, max_length=MAX_LEN, pad_to_max_length=True\n",
    "    )\n",
    "    for t in dataset[\"text\"]\n",
    "]\n",
    "labels = dataset[\"label\"]\n",
    "test_input_ids = [\n",
    "    tokenizer.encode(\n",
    "        t, add_special_tokens=True, max_length=MAX_LEN, pad_to_max_length=True\n",
    "    )\n",
    "    for t in test_dataset[\"text\"]\n",
    "]\n",
    "test_labels = test_dataset[\"label\"]\n",
    "\n",
    "# print(f\"Actual text before tokenization: {df_train_syn['text'][2]}\")\n",
    "# print(f\"Encoded input: {input_ids[2]}\")\n",
    "\n",
    "attention_masks = []\n",
    "attention_masks = [[float(i > 0) for i in seq] for seq in input_ids]\n",
    "test_attention_masks = [[float(i > 0) for i in seq] for seq in test_input_ids]\n",
    "\n",
    "#print(attention_masks[2])\n",
    "\n",
    "#nvert all our data into torch tensors, required data type for our model\n",
    "train_inputs = torch.tensor(input_ids)\n",
    "train_labels = torch.tensor(labels)\n",
    "train_masks = torch.tensor(attention_masks)\n",
    "\n",
    "\n",
    "test_inputs = torch.tensor(test_input_ids)\n",
    "test_masks = torch.tensor(test_attention_masks)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 4\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop,\n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def tokenization(example):\n",
    "#     # print(f\"I have {len(example['text'])} rows in example\")\n",
    "#     # print(example)\n",
    "\n",
    "#     for idx, row in enumerate(example[\"text\"]):\n",
    "#         example[\"text\"][idx] = f'{tokenizer.bos_token}{LABELS[example[\"label\"][idx]]}{tokenizer.sep_token}{row}{tokenizer.eos_token}'\n",
    "     \n",
    "#     out_dict = tokenizer(example[\"text\"], max_length=MAX_LEN, pad_to_max_length=True)\n",
    "\n",
    "#     out_dict[\"text\"] = example[\"text\"]\n",
    "\n",
    "#     return out_dict\n",
    "\n",
    "# BATCH_SIZE = 4\n",
    "\n",
    "# dataset = dataset.map(tokenization, batched=True,remove_columns=[\"label\"], )\n",
    "# dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# test_dataset = test_dataset.map(tokenization, batched=True,remove_columns=[\"label\"])\n",
    "# test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "# # test_dataset = test_dataset.with_format(\"torch\", devi# ce=device)\n",
    "# test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "# Convert model for DP\n",
    "model = model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, eps=1e-8)\n",
    "\n",
    "model = model.train()\n",
    "\n",
    "\n",
    "EPOCHS = 3\n",
    "LOGGING_INTERVAL = 5000 # once every how many steps we run evaluation cycle and report metrics\n",
    "EPSILON = 7.5\n",
    "DELTA = 1 / len(train_dataloader) # Parameter for privacy accounting. Probability of not achieving privacy guarantees\n",
    "MAX_GRAD_NORM = 0.1\n",
    "MAX_PHYSICAL_BATCH_SIZE = 8\n",
    "\n",
    "\n",
    "# Initialize Privacy Engine\n",
    "privacy_engine = PrivacyEngine()\n",
    "model, optimizer, data_loader = privacy_engine.make_private_with_epsilon(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader= train_dataloader,\n",
    "    target_delta=DELTA,\n",
    "    target_epsilon=EPSILON, \n",
    "    epochs=EPOCHS,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "# define evaluation cycle\n",
    "def evaluate(model):    \n",
    "    model.eval()\n",
    "\n",
    "    loss_arr = []\n",
    "    accuracy_arr = []\n",
    "    \n",
    "    for batch in test_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      }\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss, logits = outputs[:2]\n",
    "            \n",
    "            preds = np.argmax(logits.detach().cpu().numpy(), axis=1)\n",
    "            labels = inputs['labels'].detach().cpu().numpy()\n",
    "            \n",
    "            loss_arr.append(loss.item())\n",
    "            accuracy_arr.append(accuracy(preds, labels))\n",
    "    \n",
    "    model.train()\n",
    "    return np.mean(loss_arr), np.mean(accuracy_arr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Define the number of training epochs\n",
    "# num_epochs = 1  # Set the number of epochs\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch in data_loader:\n",
    "#         inputs = batch[\"input_ids\"].to(model.device)\n",
    "#         labels = batch[\"labels\"].to(model.device)  # Assuming labels are provided in the data_loader\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(inputs, labels=labels)\n",
    "#         loss = outputs.loss\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     # Track privacy budget\n",
    "#     epsilon, best_alpha = privacy_engine.get_privacy_spent(delta=1e-5)\n",
    "#     print(f\"Epoch {epoch + 1}: Spent privacy budget: ε = {epsilon}, δ = {best_alpha}\")\n",
    "\n",
    "# # Save the model\n",
    "# model.save_pretrained(\"my_private_gpt2_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b13d87907d64034a3315281dfc82f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Per sample gradient is not initialized. Not updated in backward pass?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 28\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m%\u001b[39m LOGGING_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     31\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(losses)\n",
      "File \u001b[1;32mc:\\Users\\Saad\\anaconda3\\envs\\tenv\\Lib\\site-packages\\opacus\\optimizers\\optimizer.py:513\u001b[0m, in \u001b[0;36mDPOptimizer.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m    511\u001b[0m         closure()\n\u001b[1;32m--> 513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Saad\\anaconda3\\envs\\tenv\\Lib\\site-packages\\opacus\\optimizers\\optimizer.py:494\u001b[0m, in \u001b[0;36mDPOptimizer.pre_step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpre_step\u001b[39m(\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28mself\u001b[39m, closure: Optional[Callable[[], \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    485\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    486\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;124;03m    Perform actions specific to ``DPOptimizer`` before calling\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m    underlying  ``optimizer.step()``\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;124;03m            returns the loss. Optional for most optimizers.\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 494\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_and_accumulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_skip_next_step():\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_last_step_skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Saad\\anaconda3\\envs\\tenv\\Lib\\site-packages\\opacus\\optimizers\\optimizer.py:397\u001b[0m, in \u001b[0;36mDPOptimizer.clip_and_accumulate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip_and_accumulate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    392\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;124;03m    Performs gradient clipping.\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;124;03m    Stores clipped and aggregated gradients into `p.summed_grad```\u001b[39;00m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_samples\u001b[49m[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;66;03m# Empty batch\u001b[39;00m\n\u001b[0;32m    399\u001b[0m         per_sample_clip_factor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m0\u001b[39m,))\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Saad\\anaconda3\\envs\\tenv\\Lib\\site-packages\\opacus\\optimizers\\optimizer.py:345\u001b[0m, in \u001b[0;36mDPOptimizer.grad_samples\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m ret \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams:\n\u001b[1;32m--> 345\u001b[0m     ret\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_flat_grad_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\Saad\\anaconda3\\envs\\tenv\\Lib\\site-packages\\opacus\\optimizers\\optimizer.py:282\u001b[0m, in \u001b[0;36mDPOptimizer._get_flat_grad_sample\u001b[1;34m(self, p)\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer sample gradient not found. Are you using GradSampleModule?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad_sample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer sample gradient is not initialized. Not updated in backward pass?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(p\u001b[38;5;241m.\u001b[39mgrad_sample, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    286\u001b[0m     ret \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mgrad_sample\n",
      "\u001b[1;31mValueError\u001b[0m: Per sample gradient is not initialized. Not updated in backward pass?"
     ]
    }
   ],
   "source": [
    "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    losses = []\n",
    "\n",
    "    with BatchMemoryManager(\n",
    "        data_loader=train_dataloader, \n",
    "        max_physical_batch_size=MAX_PHYSICAL_BATCH_SIZE, \n",
    "        optimizer=optimizer\n",
    "    )   as memory_safe_data_loader:\n",
    "\n",
    "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "            optimizer.zero_grad()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "            inputs = {'input_ids':    batch[0],\n",
    "                    'attention_mask': batch[1]\n",
    "    \n",
    "                    }\n",
    "\n",
    "            outputs = model(**inputs, labels=batch[0]) # output = loss, logits, hidden_states, attentions\n",
    "            # loss = optimizer(logits.view(-1, logits.shape[-1]), train_labels.view(-1))\n",
    "            loss = outputs[0]\n",
    "            loss = outputs.loss\n",
    "           \n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step > 0 and step % LOGGING_INTERVAL == 0:\n",
    "                train_loss = np.mean(losses)\n",
    "                eps = privacy_engine.get_epsilon(DELTA)\n",
    "\n",
    "                eval_loss, eval_accuracy = evaluate(model)\n",
    "\n",
    "                print(\n",
    "                  f\"Epoch: {epoch} | \"\n",
    "                  f\"Step: {step} | \"\n",
    "                  f\"Train loss: {train_loss:.3f} | \"\n",
    "                  f\"Eval loss: {eval_loss:.3f} | \"\n",
    "                  f\"Eval accuracy: {eval_accuracy:.3f} | \"\n",
    "                  f\"ɛ: {eps:.2f}\"\n",
    "                )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
